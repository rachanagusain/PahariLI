{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "PLI_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachanaGusain/PahariLI/blob/main/PLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnubs-Nnb3cw"
      },
      "source": [
        "# Language Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoUDvx56cWhQ"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvVIhx9wppqB"
      },
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOXkvuQk_JuF"
      },
      "source": [
        "import os\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from time import time\n",
        "from scipy import sparse\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.manifold import TSNE, MDS\n",
        "from collections import namedtuple, defaultdict, Counter, OrderedDict\n",
        "from itertools import tee, islice, accumulate, combinations\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCR2dz2q7XiJ"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use(\"pgf\")\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"pgf.texsystem\": 'pdflatex',\n",
        "    \"font.family\": 'serif',  # use serif/main font for text elements\n",
        "    \"text.usetex\": True,     # use inline math for ticks\n",
        "    \"pgf.rcfonts\": False,    # don't setup fonts from rc parameters\n",
        "    \"font.size\": 8,          # Use 8pt font in plots, to match 10pt font in document\n",
        "    \"axes.titlesize\": 8,\n",
        "    \"axes.labelsize\": 8,\n",
        "    \"xtick.labelsize\": 6,    # Make the legend/label fonts a little smaller\n",
        "    \"ytick.labelsize\": 6,\n",
        "    \"xtick.major.size\": 0,\n",
        "    \"ytick.major.size\": 0,\n",
        "    \"xtick.major.width\": 0.2,\n",
        "    \"ytick.major.width\": 0.2,\n",
        "    \"xtick.minor.size\" : 1.5,\n",
        "    \"xtick.minor.width\": 0.2,\n",
        "    \"xtick.direction\": 'in',\n",
        "    \"lines.markersize\": 1.2,\n",
        "    \"lines.linewidth\": 0.5,\n",
        "    \"hatch.linewidth\": 0.4,\n",
        "    \"patch.linewidth\": 0.2,\n",
        "    \"axes.prop_cycle\": matplotlib.cycler('color', 'k'),\n",
        "    \"hatch.color\": 'k',\n",
        "    \"axes.linewidth\": 0.2,\n",
        "    \"grid.linewidth\": 0.2,\n",
        "    \"legend.fontsize\": 6,\n",
        "    \"legend.title_fontsize\": 6,\n",
        "    \"legend.labelspacing\": 0.1,\n",
        "    \"legend.handlelength\": 3,\n",
        "    \"legend.frameon\": False,\n",
        "    \"savefig.dpi\": 1000,\n",
        "    \"savefig.bbox\": 'tight',\n",
        "    \"savefig.format\": 'pdf'\n",
        "    })\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBx2NeyG4vxM"
      },
      "source": [
        "! sudo apt-get install texlive-latex-recommended \n",
        "! sudo apt install texlive-latex-extra\n",
        "! sudo apt install dvipng\n",
        "! sudo apt install cm-super\n",
        "#!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1rkhLLrFLfM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3s3ZOK18oM2"
      },
      "source": [
        "num_clf = 2\n",
        "clflist = [\"mnb\", \"svm\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYqCIxzFb3c5"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q02Yj6oGxZB_"
      },
      "source": [
        "# Data files\n",
        "tr_file = \"/content/drive/MyDrive/PLI/data/train.txt\"\n",
        "ts_file = \"/content/drive/MyDrive/PLI/data/test.txt\"\n",
        "\n",
        "# Load data\n",
        "tr_data = open(tr_file, mode='r', encoding='utf-8')\n",
        "ts_data = open(ts_file, mode='r', encoding='utf-8')\n",
        "\n",
        "print(\"Data loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWeG0HRpxZB_"
      },
      "source": [
        "# Separate text and labels\n",
        "tr_text = []\n",
        "tr_lang = []\n",
        "ts_text = []\n",
        "ts_lang = []\n",
        "\n",
        "for line in tr_data:\n",
        "    text, lang = line.strip().split('\\t')\n",
        "    tr_text.append(text)\n",
        "    tr_lang.append(lang)\n",
        "\n",
        "for line in ts_data:\n",
        "    text, lang = line.strip().split('\\t')\n",
        "    ts_text.append(text)\n",
        "    ts_lang.append(lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YE576k_wyOi"
      },
      "source": [
        "label = {'dgo': 0, 'gbm': 1, 'kfy': 2, 'npi': 3}\n",
        "\n",
        "y_tr = np.asarray(list(map(lambda x: label[x], tr_lang)))\n",
        "y_ts = np.asarray(list(map(lambda x: label[x], ts_lang)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52MEhE75dhve"
      },
      "source": [
        "def size_mb(docs):\n",
        "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
        "\n",
        "tr_size_mb = size_mb(tr_text)\n",
        "ts_size_mb = size_mb(ts_text)\n",
        "\n",
        "print(\"Train data: %d sentences - %0.2f MB\" % (len(tr_text), tr_size_mb))\n",
        "print(\"Test data : %d sentences - %0.2f MB\" % (len(ts_text), ts_size_mb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxYmsfwyVI79"
      },
      "source": [
        "## Language Identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNyCTAQIizqV"
      },
      "source": [
        "def ngrams(term, ngram_range, min_df=1, max_df=1.0):\n",
        "    \"\"\"\n",
        "    Function to extract word or char n-gram features.\n",
        "\n",
        "    Parameters:\n",
        "        analyzer: string {'word', 'char', 'char_wb'}\n",
        "            Whether the feature should be made of word n-gram or character n-grams.\n",
        "            Option 'char_wb' creates character n-grams only from text inside word boundaries;\n",
        "            n-grams at the edges of words are padded with space.\n",
        "\n",
        "        ngram_range: tuple (min_n, max_n), default=(1, 1)\n",
        "            The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
        "            All values of n such that min_n <= n <= max_n will be used.\n",
        "            For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
        "\n",
        "        min_df: float in range [0.0, 1.0] or int, default=1\n",
        "            When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "            This value is also called cut-off in the literature.\n",
        "            If float, the parameter represents a proportion of documents, integer absolute counts.\n",
        "\n",
        "        max_df: float in range [0.0, 1.0] or int, default=1.0\n",
        "            When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
        "            If float, the parameter represents a proportion of documents, integer absolute counts.\n",
        "\n",
        "    Returns:\n",
        "        z: dict {features: list of features, tr_analyzer_ngram: scipy.sparse.csr.csr_matrix, ts_analyzer_ngram: scipy.sparse.csr.csr_matrix}\n",
        "    \"\"\"\n",
        "    \n",
        "    i, j = ngram_range\n",
        "    \n",
        "    def word_ngram_analyzer(doc):\n",
        "        for line in doc.split('\\n'):\n",
        "            terms = re.findall(r\"\\w+\", line)\n",
        "            for n in range(i, j+1):\n",
        "                for ngram in zip(*[islice(seq, k, len(terms)) for k, seq in enumerate(tee(terms, n))]):\n",
        "                    ngram = \" \".join(ngram)\n",
        "                    yield ngram\n",
        "    \n",
        "    try:\n",
        "        if term == 'word':\n",
        "            vectorizer = CountVectorizer(analyzer=word_ngram_analyzer, min_df=min_df, max_df=max_df)\n",
        "        else:\n",
        "            vectorizer = CountVectorizer(analyzer=term, ngram_range=(i, j), min_df=min_df, max_df=max_df)\n",
        "        vectorizer.fit(tr_text)\n",
        "    except ValueError:\n",
        "        print(\"Error: After pruning, no terms remain.\")\n",
        "        return None\n",
        "\n",
        "    z = dict()\n",
        "    z[\"features\"] = vectorizer.get_feature_names()\n",
        "    z[\"tr_\"+term+\"_\"+str(i)+str(j)] = vectorizer.transform(tr_text)\n",
        "    z[\"ts_\"+term+\"_\"+str(i)+str(j)] = vectorizer.transform(ts_text)\n",
        "\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ILlhY8Ykph"
      },
      "source": [
        "def classifier(clf, params, X_tr, y_tr, X_ts, y_ts):\n",
        "    print(\"Training\", clf)\n",
        "\n",
        "    search = GridSearchCV(estimator=clf, param_grid=params, scoring='accuracy', \n",
        "                          cv=5, verbose=1, return_train_score=True)\n",
        "    t0 = time()\n",
        "    search.fit(X_tr, y_tr)\n",
        "    tr_val_time = time() - t0\n",
        "    clf = search.best_estimator_\n",
        "    print(\"\\nBest Estimator:\", clf)\n",
        "    print(\"\\nTrain and validation time: %.4f seconds\" % tr_val_time)\n",
        "\n",
        "    t0 = time()\n",
        "    y_true, y_pred = y_ts, clf.predict(X_ts)\n",
        "    ts_time = time() - t0\n",
        "    print(\"\\nTest time: %.4f seconds\" % ts_time)\n",
        "\n",
        "    confusion_mat = metrics.confusion_matrix(y_true, y_pred)\n",
        "    scores_report = metrics.classification_report(y_true, y_pred, target_names=label.keys(), output_dict=True)\n",
        "    scores_report = pd.DataFrame(scores_report)\n",
        "    \n",
        "    print(metrics.classification_report(y_true, y_pred, target_names=label.keys(), digits=4))\n",
        "    cm_disp = metrics.plot_confusion_matrix(clf, X_ts, y_ts, values_format='d', \n",
        "                                            display_labels=label.keys(),\n",
        "                                            cmap=plt.cm.Blues, colorbar=False)\n",
        "    plt.show()\n",
        "\n",
        "    return search.best_params_, tr_val_time, ts_time, scores_report, cm_disp, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMVKEZ-S9SkC"
      },
      "source": [
        "def build_ngram_model(max_n):\n",
        "    ngram = range(1, max_n+1)\n",
        "    ngram_range = [(i, j) for i in ngram for j in ngram if i<=j]\n",
        "\n",
        "    outputs = {clf: dict() for clf in clflist}\n",
        "    results = {clf: [] for clf in clflist}\n",
        "    columns = [\"Vectorizer\", \"#Features\", \"Hyperparameter\", \n",
        "               \"Train&Val time (s)\", \"Test time (s)\", \n",
        "               \"Precision\", \"Recall\", \"F1-score\", \"Accuracy\"]\n",
        "\n",
        "    for analyzer in ['word', 'char', 'char_wb']:\n",
        "        for (i, j) in ngram_range:\n",
        "            print('*'*80)\n",
        "            print(f\"Extracting frequency based {analyzer} n-gram features...\")\n",
        "            z = ngrams(term=analyzer, ngram_range=(i, j), min_df=0.005)\n",
        "            if z is None:          \n",
        "                continue\n",
        "            \n",
        "            vect = analyzer+\"_\"+str(i)+str(j)\n",
        "            X_tr = z[\"tr_\"+vect].toarray()\n",
        "            X_ts = z[\"ts_\"+vect].toarray()\n",
        "\n",
        "            for (clf, params, descript), clfname in zip([\n",
        "                    (MultinomialNB(), \n",
        "                     {'alpha': np.power(10, np.arange(-3, 2, dtype=float))}, \n",
        "                     \"Multinomial NaÃ¯ve Bayes Classifier\"),\n",
        "                    (LinearSVC(dual=False), \n",
        "                     {'C': np.power(10, np.arange(-3, 2, dtype=float))}, \n",
        "                     \"Linear Support Vector Classifier\")], \n",
        "                    clflist):\n",
        "                best_param, tr_val_time, ts_time, scores, cm_disp, y_pred = classifier(clf, params, X_tr, y_tr, X_ts, y_ts)\n",
        "                cm_df = pd.DataFrame(cm_disp.confusion_matrix, index=label.keys(), columns=label.keys())\n",
        "                cm_df.to_csv(os.path.join(dirpath, 'confusion_matrix', 'cm_'+clfname+'_'+vect+'.csv'))\n",
        "                result = []\n",
        "                result.extend([vect, len(z[\"features\"])])          \n",
        "                result.extend(best_param.values())                 \n",
        "                result.extend([tr_val_time, ts_time])               \n",
        "                result.extend(scores[\"macro avg\"].tolist()[:-1])   \n",
        "                result.append(scores[\"accuracy\"].iat[0]*100)\n",
        "                results[clfname].append(dict(zip(columns, result)))\n",
        "                outputs[clfname][vect] = y_pred\n",
        "\n",
        "    return {clf: pd.DataFrame(results[clf]) for clf in clflist}, outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpcfqrgxtvBK"
      },
      "source": [
        "dirpath = \"/content/drive/MyDrive/PLI/results\"\n",
        "results, outputs = build_ngram_model(8)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}